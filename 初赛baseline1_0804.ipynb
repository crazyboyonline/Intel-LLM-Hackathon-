{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127a3a72-05e4-4336-b0ee-4856bf481248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T02:03:38.004105Z",
     "iopub.status.busy": "2024-06-22T02:03:38.003588Z",
     "iopub.status.idle": "2024-06-22T02:03:38.007672Z",
     "shell.execute_reply": "2024-06-22T02:03:38.007017Z",
     "shell.execute_reply.started": "2024-06-22T02:03:38.004078Z"
    }
   },
   "source": [
    "# ç®€ä»‹\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰,æˆ‘ä»¬å¯ä»¥å…ˆè®¤è¯†ä¸€ä¸‹ä»€ä¹ˆæ˜¯ IPEX-LLM, IPEX-LLMæ˜¯ä¸€ä¸ªPyTorchåº“ï¼Œç”¨äºåœ¨Intel CPUå’ŒGPUï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰iGPUçš„æœ¬åœ°PC,Arcã€Flexå’ŒMaxç­‰ç‹¬ç«‹GPUï¼‰ä¸Šä»¥éå¸¸ä½çš„å»¶è¿Ÿè¿è¡ŒLLM.æ€»è€Œè¨€ä¹‹æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒåŠ å¿«å¤§è¯­è¨€æ¨¡å‹åœ¨ intel ç”Ÿæ€è®¾å¤‡ä¸Šçš„è¿è¡Œé€Ÿåº¦;æ— éœ€é¢å¤–è´­ä¹°å…¶ä»–è®¡ç®—è®¾å¤‡,æˆ‘ä»¬å¯ä»¥é«˜é€Ÿç‡ä½æ¶ˆè€—çš„æ–¹å¼åœ¨æœ¬åœ°ç”µè„‘ä¸Šè¿è¡Œå¤§è¯­è¨€æ¨¡å‹.\n",
    "\n",
    "åœ¨æœ¬æ¬¡æ¯”èµ›çš„ç¬¬ä¸€ç¯‡æ•™ç¨‹ä¸­,æˆ‘ä»¬å°±èƒ½æŒæ¡ IPEX-LLM çš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•,æˆ‘ä»¬å°†åˆ©ç”¨ IPEX-LLM åŠ é€Ÿ Qwen2 è¯­è¨€æ¨¡å‹çš„è¿è¡Œ,è·Ÿéšè¿™ç¯‡ notebook ä¸€æ­¥æ­¥ä»”ç»†æ“ä½œ,æˆ‘ä»¬å¯ä»¥ç®€å•å¿«é€Ÿçš„æŒæ¡å¤§è¯­è¨€æ¨¡å‹åœ¨ intel ç¡¬ä»¶ä¸Šçš„é«˜æ€§èƒ½éƒ¨ç½².\n",
    "\n",
    "# ä¸€ã€å®‰è£…ç¯å¢ƒ\n",
    "\n",
    "åœ¨å¼€å§‹è¿è¡Œæ¨ç†ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½è¿è¡Œ qwen2 éœ€è¦çš„å¿…é¡»ç¯å¢ƒï¼Œæ­¤æ—¶è¯·ç¡®ä¿ä½ è¿›å…¥çš„é•œåƒæ˜¯ `ubuntu22.04-py310-torch2.1.2-tf2.14.0-1.14.0` å¦åˆ™å°†ä¼šçœ‹åˆ°æ‰¾ä¸åˆ° conda æ–‡ä»¶å¤¹çš„æŠ¥é”™ï¼Œåˆ‡è®°ã€‚\n",
    "\n",
    "ä½ å°†åœ¨ç»ˆç«¯è¿è¡Œä¸‹åˆ—è„šæœ¬,è¿›è¡Œ ipex-llm çš„æ­£å¼ conda ç¯å¢ƒçš„æ¢å¤ï¼Œæ¢å¤å®Œæˆåå…³é—­æ‰€æœ‰å¼€å¯çš„ notebook çª—å£ï¼Œç„¶åé‡æ–°æ‰“å¼€ï¼Œæ‰èƒ½æ­£å¸¸åˆ‡æ¢å¯¹åº” kernelã€‚\n",
    "\n",
    "é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯ kernel å‘¢ï¼Ÿç®€å•ç†è§£ï¼Œå®ƒç”¨äºæä¾› python\n",
    "ä»£ç è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰æ”¯æŒï¼Œè€Œä¼šæŠŠæˆ‘ä»¬çš„æ¶ˆæ¯å‘é€åˆ°å¯¹åº”çš„ kernel è¿›è¡Œæ‰§è¡Œã€‚ä½ å¯ä»¥åœ¨ notebook å³ä¸Šè§’çœ‹åˆ° Python3(ipykernel) çš„å­—æ ·ï¼Œå®ƒä»£è¡¨é»˜è®¤ç¯å¢ƒçš„å†…æ ¸ï¼›æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å¯¹åº”è™šæ‹Ÿç¯å¢ƒå¯åŠ¨ jupyter notebook ä½¿ç”¨å¯¹åº”è™šæ‹Ÿç¯å¢ƒçš„å†…æ ¸ç¯å¢ƒï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç±»ä¼¼ `python3 -m ipykernel install --name=ipex` çš„æŒ‡ä»¤å°†æŸä¸ªè™šæ‹Ÿç¯å¢ƒï¼ˆåœ¨è¿™é‡Œæ˜¯ ipexï¼‰æ³¨å†Œåˆ° notebook çš„å¯ä½¿ç”¨å†…æ ¸ä¸­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90956f90-fc56-4e3b-bb23-b99fb63afeb3",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-04T18:25:27.195710Z",
     "iopub.status.busy": "2024-08-04T18:25:27.195414Z",
     "iopub.status.idle": "2024-08-04T18:25:27.202183Z",
     "shell.execute_reply": "2024-08-04T18:25:27.201794Z",
     "shell.execute_reply.started": "2024-08-04T18:25:27.195692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/cs/Desktop/LLM&Datawhale/åŸºäºLLMçš„ç‰›å£è¹„ç–«æ™ºèƒ½è¯Šæ–­åŠ©æ‰‹/LLM/install.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /Users/cs/Desktop/LLM&Datawhale/åŸºäºLLMçš„ç‰›å£è¹„ç–«æ™ºèƒ½è¯Šæ–­åŠ©æ‰‹/LLM/install.sh\n",
    "# åˆ‡æ¢åˆ° conda çš„ç¯å¢ƒæ–‡ä»¶å¤¹\n",
    "cd  /opt/conda/envs \n",
    "mkdir ipex\n",
    "# ä¸‹è½½ ipex-llm å®˜æ–¹ç¯å¢ƒ\n",
    "wget https://s3.idzcn.com/ipex-llm/ipex-llm-2.1.0b20240410.tar.gz \n",
    "# è§£å‹æ–‡ä»¶å¤¹ä»¥ä¾¿æ¢å¤åŸå…ˆç¯å¢ƒ\n",
    "tar -zxvf ipex-llm-2.1.0b20240410.tar.gz -C ipex/ && rm ipex-llm-2.1.0b20240410.tar.gz\n",
    "# å®‰è£… ipykernel å¹¶å°†å…¶æ³¨å†Œåˆ° notebook å¯ä½¿ç”¨å†…æ ¸ä¸­\n",
    "/opt/conda/envs/ipex/bin/python3 -m pip install ipykernel && /opt/conda/envs/ipex/bin/python3 -m ipykernel install --name=ipex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a75102-fb8f-4492-926d-393155c5482f",
   "metadata": {},
   "source": [
    "å½“ä½ è¿è¡Œå®Œä¸Šé¢çš„ä»£ç å—åï¼Œæ­¤æ—¶ä¼šåœ¨ `/mnt/workspace` ç›®å½•ä¸‹åˆ›å»ºåä¸º `install.sh` åå­—çš„ bash è„šæœ¬ï¼Œä½ éœ€è¦æ‰“å¼€ç»ˆç«¯ï¼Œæ‰§è¡Œå‘½ä»¤ `bash install.sh` è¿è¡Œ bash è„šæœ¬ï¼Œç­‰å¾…æ‰§è¡Œå®Œæ¯•åå…³é—­æ‰€æœ‰çš„ notebook çª—å£å†é‡æ–°æ‰“å¼€ï¼Œç›´åˆ°ä½ åœ¨å³ä¸Šè§’ç‚¹å‡» `Python3 (ipykernel)` åå¯ä»¥çœ‹åˆ°åä¸º `ipex` çš„ç¯å¢ƒï¼Œç‚¹å‡»ååˆ‡æ¢å³å¯è¿›å…¥åˆ° `ipex-llm` çš„æ­£å¼å¼€å‘ç¯å¢ƒï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ç»ˆç«¯ä¸­æ‰§è¡Œ `conda activate ipex` å¯åŠ¨ ipex çš„è™šæ‹Ÿç¯å¢ƒï¼Œè‡³æ­¤å‡†å¤‡å·¥ä½œå®Œæˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b278f-49a7-4ee8-bc79-834622fa2753",
   "metadata": {},
   "source": [
    "# äºŒã€æ¨¡å‹å‡†å¤‡\n",
    "\n",
    "Qwen2æ˜¯é˜¿é‡Œäº‘æœ€æ–°æ¨å‡ºçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œç›¸æ¯”Qwen1.5ï¼ŒQwen2å®ç°äº†æ•´ä½“æ€§èƒ½çš„ä»£é™…é£è·ƒï¼Œå¤§å¹…æå‡äº†ä»£ç ã€æ•°å­¦ã€æ¨ç†ã€æŒ‡ä»¤éµå¾ªã€å¤šè¯­è¨€ç†è§£ç­‰èƒ½åŠ›ã€‚\n",
    "\n",
    "åŒ…å«5ä¸ªå°ºå¯¸çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šQwen2-0.5Bã€Qwen2-1.5Bã€Qwen2-7Bã€Qwen2-57B-A14Bå’ŒQwen2-72Bï¼Œå…¶ä¸­Qwen2-57B-A14Bä¸ºæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ã€‚æ‰€æœ‰å°ºå¯¸æ¨¡å‹éƒ½ä½¿ç”¨äº†GQAï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰æœºåˆ¶ï¼Œä»¥ä¾¿è®©ç”¨æˆ·ä½“éªŒåˆ°GQAå¸¦æ¥çš„æ¨ç†åŠ é€Ÿå’Œæ˜¾å­˜å ç”¨é™ä½çš„ä¼˜åŠ¿ã€‚\n",
    "\n",
    "åœ¨ä¸­æ–‡ã€è‹±è¯­çš„åŸºç¡€ä¸Šï¼Œè®­ç»ƒæ•°æ®ä¸­å¢åŠ äº†27ç§è¯­è¨€ç›¸å…³çš„é«˜è´¨é‡æ•°æ®ã€‚å¢å¤§äº†ä¸Šä¸‹æ–‡é•¿åº¦æ”¯æŒï¼Œæœ€é«˜è¾¾åˆ°128K tokensï¼ˆQwen2-72B-Instructï¼‰ã€‚\n",
    "\n",
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `Qwen/Qwen2-1.5B-Instruct` çš„æ¨¡å‹å‚æ•°ç‰ˆæœ¬æ¥ä½“éªŒ Qwen2 çš„å¼ºå¤§èƒ½åŠ›ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¸‹è½½ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ modelscope çš„ api å¾ˆå®¹æ˜“å®ç°æ¨¡å‹çš„ä¸‹è½½ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca434513-e0b1-42f4-bdbf-21e3ed9a2af6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:15:18.231123Z",
     "iopub.status.busy": "2024-07-26T04:15:18.230928Z",
     "iopub.status.idle": "2024-07-26T04:15:21.206791Z",
     "shell.execute_reply": "2024-07-26T04:15:21.206240Z",
     "shell.execute_reply.started": "2024-07-26T04:15:18.231107Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 12:15:19,492 - modelscope - INFO - PyTorch version 2.2.2 Found.\n",
      "2024-07-26 12:15:19,493 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-07-26 12:15:19,531 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-07-26 12:15:19,570 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 0f29b277c8f0d5b4256adb7dfe1f0397 and a total number of 972 components indexed\n",
      "/opt/conda/envs/ipex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "# ç¬¬ä¸€ä¸ªå‚æ•°è¡¨ç¤ºä¸‹è½½æ¨¡å‹çš„å‹å·ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸‹è½½åå­˜æ”¾çš„ç¼“å­˜åœ°å€ï¼Œç¬¬ä¸‰ä¸ªè¡¨ç¤ºç‰ˆæœ¬å·ï¼Œé»˜è®¤ master\n",
    "model_dir = snapshot_download('Qwen/Qwen2-1.5B-Instruct', cache_dir='qwen2chat_src', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d647d5a-a675-4ddf-8cd6-ccabd0b5140e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T10:59:26.623781Z",
     "iopub.status.busy": "2024-06-22T10:59:26.623253Z",
     "iopub.status.idle": "2024-06-22T10:59:26.627138Z",
     "shell.execute_reply": "2024-06-22T10:59:26.626571Z",
     "shell.execute_reply.started": "2024-06-22T10:59:26.623758Z"
    }
   },
   "source": [
    "ä¸‹è½½å®Œæˆåï¼Œæˆ‘ä»¬å°†å¯¹ qwen2 æ¨¡å‹è¿›è¡Œä½ç²¾åº¦é‡åŒ–è‡³ int4 ï¼Œä½ç²¾åº¦é‡åŒ–ï¼ˆLow Precision Quantizationï¼‰æ˜¯æŒ‡å°†æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ä½å®½çš„æ•´æ•°ï¼ˆè¿™é‡Œæ˜¯int4ï¼‰ï¼Œä»¥å‡å°‘è®¡ç®—èµ„æºçš„éœ€æ±‚å’Œæé«˜ç³»ç»Ÿçš„æ•ˆç‡ã€‚è¿™ç§æŠ€æœ¯åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­å°¤å…¶é‡è¦ï¼Œå®ƒå¯ä»¥åœ¨ç¡¬ä»¶ä¸Šå®ç°å¿«é€Ÿã€ä½åŠŸè€—çš„æ¨ç†ï¼Œä¹Ÿå¯ä»¥åŠ å¿«æ¨¡å‹åŠ è½½çš„é€Ÿåº¦ã€‚\n",
    "\n",
    "ç»è¿‡ Intel ipex-llm ä¼˜åŒ–åçš„å¤§æ¨¡å‹åŠ è½½ api `from ipex_llm.transformers import AutoModelForCausalLM`ï¼Œ æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“é€šè¿‡ `load_in_low_bit='sym_int4'` å°†æ¨¡å‹é‡åŒ–åˆ° int4 ï¼Œè‹±ç‰¹å°” IPEX-LLM æ”¯æŒ â€˜sym_int4â€™, â€˜asym_int4â€™, â€˜sym_int5â€™, â€˜asym_int5â€™ æˆ– 'sym_int8â€™é€‰é¡¹ï¼Œå…¶ä¸­ â€˜symâ€™ å’Œ â€˜asymâ€™ ç”¨äºåŒºåˆ†å¯¹ç§°é‡åŒ–ä¸éå¯¹ç§°é‡åŒ–ã€‚ æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `save_low_bit` api å°†è½¬æ¢åçš„æ¨¡å‹æƒé‡ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7946f4-ad7e-4feb-bca9-6a44ca7bfe15",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:15:25.551721Z",
     "iopub.status.busy": "2024-07-26T04:15:25.551342Z",
     "iopub.status.idle": "2024-07-26T04:15:36.203840Z",
     "shell.execute_reply": "2024-07-26T04:15:36.203238Z",
     "shell.execute_reply.started": "2024-07-26T04:15:25.551700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 12:15:27,226 - INFO - Converting the current model to sym_int4 format......\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "from transformers import  AutoTokenizer\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    model_path = os.path.join(os.getcwd(),\"qwen2chat_src/Qwen/Qwen2-1___5B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit='sym_int4', trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model.save_low_bit('qwen2chat_int4')\n",
    "    tokenizer.save_pretrained('qwen2chat_int4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2285dc-cf33-4ffb-a498-5d67c82b4806",
   "metadata": {},
   "source": [
    "å‡†å¤‡å®Œè½¬æ¢åçš„é‡åŒ–æƒé‡ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†åœ¨ç»ˆç«¯ä¸­ç¬¬ä¸€æ¬¡è¿è¡Œ qwen2 åœ¨ CPU ä¸Šçš„å¤§æ¨¡å‹æ¨ç†ï¼Œä½†è¯·æ³¨æ„ä¸è¦åœ¨ notebook ä¸­è¿è¡Œï¼ˆæœ¬åœ°è¿è¡Œå¯ä»¥åœ¨ notebook ä¸­è¿è¡Œï¼Œç”±äºé­”æ­ notebook å’Œç»ˆç«¯è¿è¡Œè„šæœ¬æœ‰ä¸€äº›åŒºåˆ«ï¼Œè¿™é‡Œæ¨èåœ¨ç»ˆç«¯ä¸­è¿è¡Œã€‚\n",
    "\n",
    "åœ¨è¿è¡Œä¸‹åˆ—ä»£ç å—åï¼Œå°†ä¼šè‡ªåŠ¨åœ¨ç»ˆç«¯ä¸­æ–°å»ºä¸€ä¸ªpythonæ–‡ä»¶ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨ç»ˆç«¯è¿è¡Œè¿™ä¸ªpythonæ–‡ä»¶å³å¯å¯åŠ¨æ¨ç†ï¼š\n",
    "\n",
    "```python\n",
    "cd /mnt/workspace\n",
    "conda activate ipex\n",
    "python3 run.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0590d50a-7c85-4674-8db8-acdbaa758efd",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:16:12.529557Z",
     "iopub.status.busy": "2024-07-26T04:16:12.528821Z",
     "iopub.status.idle": "2024-07-26T04:16:12.534525Z",
     "shell.execute_reply": "2024-07-26T04:16:12.533963Z",
     "shell.execute_reply.started": "2024-07-26T04:16:12.529518Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/workspace/run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run.py\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import os\n",
    "# è®¾ç½®OpenMPçº¿ç¨‹æ•°ä¸º8,ä¼˜åŒ–CPUå¹¶è¡Œè®¡ç®—æ€§èƒ½\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "import torch\n",
    "import time\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹åŠ è½½è·¯å¾„\n",
    "load_path = \"qwen2chat_int4\"\n",
    "# åŠ è½½ä½ä½(int4)é‡åŒ–æ¨¡å‹,trust_remote_code=Trueå…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç \n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "# åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "# å®šä¹‰è¾“å…¥prompt\n",
    "prompt = \"ç»™æˆ‘è®²ä¸€ä¸ªèŠ¯ç‰‡åˆ¶é€ çš„æµç¨‹\"\n",
    "\n",
    "# æ„å»ºç¬¦åˆæ¨¡å‹è¾“å…¥æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# ä½¿ç”¨æ¨ç†æ¨¡å¼,å‡å°‘å†…å­˜ä½¿ç”¨å¹¶æé«˜æ¨ç†é€Ÿåº¦\n",
    "with torch.inference_mode():\n",
    "    # åº”ç”¨èŠå¤©æ¨¡æ¿,å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼çš„æ–‡æœ¬\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥å¼ é‡,å¹¶ç§»è‡³CPU (å¦‚æœä½¿ç”¨GPU,è¿™é‡Œåº”æ”¹ä¸º.to('cuda'))\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('cpu')\n",
    "\n",
    "    st = time.time()\n",
    "    # ç”Ÿæˆå›ç­”,max_new_tokensé™åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•°\n",
    "    generated_ids = model.generate(model_inputs.input_ids,\n",
    "                                   max_new_tokens=512)\n",
    "    end = time.time()\n",
    "\n",
    "    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨,ç”¨äºå­˜å‚¨å¤„ç†åçš„generated_ids\n",
    "    processed_generated_ids = []\n",
    "\n",
    "    # ä½¿ç”¨zipå‡½æ•°åŒæ—¶éå†model_inputs.input_idså’Œgenerated_ids\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # è®¡ç®—è¾“å…¥åºåˆ—çš„é•¿åº¦\n",
    "        input_length = len(input_ids)\n",
    "        \n",
    "        # ä»output_idsä¸­æˆªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "        # è¿™æ˜¯é€šè¿‡åˆ‡ç‰‡æ“ä½œå®Œæˆçš„,åªä¿ç•™input_lengthä¹‹åçš„éƒ¨åˆ†\n",
    "        new_tokens = output_ids[input_length:]\n",
    "        \n",
    "        # å°†æ–°ç”Ÿæˆçš„tokenæ·»åŠ åˆ°å¤„ç†åçš„åˆ—è¡¨ä¸­\n",
    "        processed_generated_ids.append(new_tokens)\n",
    "\n",
    "    # å°†å¤„ç†åçš„åˆ—è¡¨èµ‹å€¼å›generated_ids\n",
    "    generated_ids = processed_generated_ids\n",
    "\n",
    "    # è§£ç æ¨¡å‹è¾“å‡º,è½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # æ‰“å°æ¨ç†æ—¶é—´\n",
    "    print(f'Inference time: {end-st:.2f} s')\n",
    "    # æ‰“å°åŸå§‹prompt\n",
    "    print('-'*20, 'Prompt', '-'*20)\n",
    "    print(text)\n",
    "    # æ‰“å°æ¨¡å‹ç”Ÿæˆçš„è¾“å‡º\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c5eb2b-e2c8-41f8-8ffa-c03968d63722",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬æ¼”ç¤ºçš„æ˜¯ç­‰åˆ°ç»“æœå®Œå…¨è¾“å‡ºåå†æ‰“å°çš„æ¨¡å¼ï¼Œä½†æœ‰èªæ˜çš„åŒå­¦è‚¯å®šå¥½å¥‡ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•èƒ½å¤Ÿè®©æˆ‘ä»¬åŠæ—¶çœ‹åˆ°è¾“å‡ºçš„ç»“æœï¼Ÿè¿™é‡Œæˆ‘ä»¬ä»‹ç»ä¸€ç§æ–°çš„è¾“å‡ºæ¨¡å¼â€”â€”æµå¼è¾“å‡ºï¼Œæµå¼çš„æ„æ€é¡¾åæ€ä¹‰å°±æ˜¯è¾“å‡ºæ˜¯ä¸æ–­æµåŠ¨çš„ï¼Œä¹Ÿå°±æ˜¯ä¸åœçš„å‘å¤–è¾“å‡ºçš„ã€‚é€šè¿‡æµå¼è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åŠæ—¶çœ‹åˆ°æ¨¡å‹è¾“å‡ºçš„ç»“æœã€‚åœ¨ transformers ä¸­ï¼Œæˆ‘ä»¬å°†ä¼šä½¿ç”¨ `TextStreamer` ç»„ä»¶æ¥å®ç°æµå¼è¾“å‡ºï¼Œè®°å¾—è¿™ä¸ª python æ–‡ä»¶åŒæ ·éœ€è¦åœ¨ç»ˆç«¯æ‰§è¡Œï¼š\n",
    "```python\n",
    "cd /mnt/workspace\n",
    "conda activate ipex\n",
    "python3 run_stream.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a82251-bd77-4f9b-ac7c-00edb1ccdb6e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:16:21.556183Z",
     "iopub.status.busy": "2024-07-26T04:16:21.555834Z",
     "iopub.status.idle": "2024-07-26T04:16:21.560294Z",
     "shell.execute_reply": "2024-07-26T04:16:21.559748Z",
     "shell.execute_reply.started": "2024-07-26T04:16:21.556165Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/workspace/run_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run_stream.py\n",
    "# è®¾ç½®OpenMPçº¿ç¨‹æ•°ä¸º8\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# å¯¼å…¥Intelæ‰©å±•çš„Transformersæ¨¡å‹\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# åŠ è½½æ¨¡å‹è·¯å¾„\n",
    "load_path = \"qwen2chat_int4\"\n",
    "\n",
    "# åŠ è½½4ä½é‡åŒ–çš„æ¨¡å‹\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "\n",
    "# åŠ è½½å¯¹åº”çš„tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "# åˆ›å»ºæ–‡æœ¬æµå¼è¾“å‡ºå™¨\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# è®¾ç½®æç¤ºè¯\n",
    "prompt = \"ç»™æˆ‘è®²ä¸€ä¸ªèŠ¯ç‰‡åˆ¶é€ çš„æµç¨‹\"\n",
    "\n",
    "# æ„å»ºæ¶ˆæ¯åˆ—è¡¨\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "# ä½¿ç”¨æ¨ç†æ¨¡å¼\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # åº”ç”¨èŠå¤©æ¨¡æ¿,æ·»åŠ ç”Ÿæˆæç¤º\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    print(\"start generate\")\n",
    "    st = time.time()  # è®°å½•å¼€å§‹æ—¶é—´\n",
    "    \n",
    "    # ç”Ÿæˆæ–‡æœ¬\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,  # æœ€å¤§ç”Ÿæˆ512ä¸ªæ–°token\n",
    "        streamer=streamer,   # ä½¿ç”¨æµå¼è¾“å‡º\n",
    "    )\n",
    "    \n",
    "    end = time.time()  # è®°å½•ç»“æŸæ—¶é—´\n",
    "    \n",
    "    # æ‰“å°æ¨ç†æ—¶é—´\n",
    "    print(f'Inference time: {end-st} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85226459-6228-4c7f-9f1f-c9d7d95d97f3",
   "metadata": {},
   "source": [
    "æ­å–œä½ ï¼Œä½ å·²ç»å®Œå…¨æŒæ¡äº†å¦‚ä½•åº”ç”¨è‹±ç‰¹å°” ipex-llm å·¥å…·åœ¨ CPU ä¸Šå®ç° qwen2 å¤§æ¨¡å‹é«˜æ€§èƒ½æ¨ç†ã€‚è‡³æ­¤å·²æŒæ¡äº†å®Œæ•´è¾“å‡º / ç”Ÿæˆæµå¼è¾“å‡ºçš„è°ƒç”¨æ–¹æ³•ï¼›æ¥ä¸‹æ¥æˆ‘ä»¬è®²æ›´è¿›ä¸€æ­¥ï¼Œé€šè¿‡ Gradio å®ç°ä¸€ä¸ªç®€å•çš„å‰ç«¯æ¥ä¸æˆ‘ä»¬åœ¨ cpu ä¸Šéƒ¨ç½²åçš„å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯ï¼Œå¹¶å®ç°æµå¼æ‰“å°è¿”å›ç»“æœã€‚\n",
    "\n",
    "Gradio æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“ï¼Œç”¨äºå¿«é€Ÿæ„å»ºæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦æ¼”ç¤ºåº”ç”¨ã€‚å®ƒä½¿å¾—å¼€å‘è€…å¯ä»¥åœ¨å‡ è¡Œä»£ç ä¸­åˆ›å»ºä¸€ä¸ªç®€å•ã€å¯è°ƒæ•´çš„ç”¨æˆ·ç•Œé¢ï¼Œç”¨äºå±•ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹æˆ–æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹ã€‚Gradio æ”¯æŒå¤šç§è¾“å…¥è¾“å‡ºç»„ä»¶ï¼Œå¦‚æ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘ã€éŸ³é¢‘ç­‰ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°åˆ†äº«åº”ç”¨ï¼ŒåŒ…æ‹¬åœ¨äº’è”ç½‘ä¸Šåˆ†äº«å’Œåœ¨å±€åŸŸç½‘å†…åˆ†äº«.\n",
    "\n",
    "ç®€å•æ¥è¯´,åˆ©ç”¨ Graio åº“,æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“å®ç°ä¸€ä¸ªå…·æœ‰å¯¹è¯åŠŸèƒ½çš„å‰ç«¯é¡µé¢.\n",
    "\n",
    "æ³¨æ„! åœ¨è¿è¡Œä¹‹å‰,æˆ‘ä»¬éœ€è¦å®‰è£… gradio åº“ä¾èµ–ç¯å¢ƒ, ä½ éœ€è¦åœ¨ç»ˆç«¯æ‰§è¡Œ:\n",
    "\n",
    "```bash\n",
    "cd /mnt/workspace\n",
    "conda activate ipex\n",
    "pip install gradio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd73a31-0feb-4fd7-ad73-9e086725a620",
   "metadata": {},
   "source": [
    "éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œåœ¨è¿è¡Œä¹‹å‰æˆ‘ä»¬è¿˜éœ€è¦å¯¹å¯åŠ¨å‘½ä»¤è¿›è¡Œä¿®æ”¹æ‰èƒ½æ­£å¸¸ä½¿ç”¨ gradio å‰ç«¯, æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ€åä¸€å¥ gradio çš„å¯åŠ¨å‘½ä»¤ ` demo.launch(root_path='/dsw-525085/proxy/7860/')` ,ä½†æ¯ä¸ªäººå¯¹åº”çš„ä¸éƒ½æ˜¯ dsw-525085,ä¹Ÿè®¸æ˜¯ dsw-233333, è¿™å–å†³äºæ­¤æ—¶ä½ çš„ç½‘é¡µ url é“¾æ¥ä¸Šæ˜¾ç¤ºçš„åœ°å€æ˜¯å¦æ˜¯ç±»ä¼¼ `https://dsw-gateway-cn-hangzhou.data.aliyun.com/dsw-525085/` çš„å­—çœ¼,æ ¹æ®ä½ æ˜¾ç¤º url çš„å¯¹åº”æ•°å­—ä¸åŒ,ä½ éœ€è¦æŠŠä¸‹é¢çš„ gradio ä»£ç  root_path ä¸­çš„ dswæ ‡è¯†ä¿®æ”¹ä¸ºæ­£ç¡®å¯¹åº”çš„æ•°å­—,æ‰èƒ½åœ¨è¿è¡Œåçœ‹åˆ°æ­£ç¡®çš„ gradio é¡µé¢.\n",
    "\n",
    "åœ¨ä¿®æ”¹å®Œ root_path å,æˆ‘ä»¬å¯ä»¥åœ¨ç»ˆç«¯ä¸­é¡ºåˆ©è¿è¡Œ gradio çª—å£:\n",
    "```python\n",
    "cd /mnt/workspace\n",
    "conda activate ipex\n",
    "python3 run_gradio_stream.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24eb15d1-5e5a-4468-a1b4-8925f9bdf772",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:16:27.082446Z",
     "iopub.status.busy": "2024-07-26T04:16:27.082041Z",
     "iopub.status.idle": "2024-07-26T04:16:27.087344Z",
     "shell.execute_reply": "2024-07-26T04:16:27.086830Z",
     "shell.execute_reply.started": "2024-07-26T04:16:27.082425Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/workspace/run_gradio_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run_gradio_stream.py\n",
    "import gradio as gr\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from threading import Thread, Event\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  # è®¾ç½®OpenMPçº¿ç¨‹æ•°ä¸º8,ç”¨äºæ§åˆ¶å¹¶è¡Œè®¡ç®—\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "load_path = \"qwen2chat_int4\"  # æ¨¡å‹è·¯å¾„\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)  # åŠ è½½ä½ä½æ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)  # åŠ è½½å¯¹åº”çš„tokenizer\n",
    "\n",
    "# å°†æ¨¡å‹ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨\n",
    "model = model.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°é€‰å®šçš„è®¾å¤‡ä¸Š\n",
    "\n",
    "# åˆ›å»º TextIteratorStreamerï¼Œç”¨äºæµå¼ç”Ÿæˆæ–‡æœ¬\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªåœæ­¢äº‹ä»¶ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹çš„ä¸­æ–­\n",
    "stop_event = Event()\n",
    "\n",
    "# å®šä¹‰ç”¨æˆ·è¾“å…¥å¤„ç†å‡½æ•°\n",
    "def user(user_message, history):\n",
    "    return \"\", history + [[user_message, None]]  # è¿”å›ç©ºå­—ç¬¦ä¸²å’Œæ›´æ–°åçš„å†å²è®°å½•\n",
    "\n",
    "# å®šä¹‰æœºå™¨äººå›å¤ç”Ÿæˆå‡½æ•°\n",
    "def bot(history):\n",
    "    stop_event.clear()  # é‡ç½®åœæ­¢äº‹ä»¶\n",
    "    prompt = history[-1][0]  # è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]  # æ„å»ºæ¶ˆæ¯æ ¼å¼\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)  # åº”ç”¨èŠå¤©æ¨¡æ¿\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)  # å¯¹è¾“å…¥è¿›è¡Œç¼–ç å¹¶ç§»åˆ°æŒ‡å®šè®¾å¤‡\n",
    "    \n",
    "    print(f\"\\nç”¨æˆ·è¾“å…¥: {prompt}\")\n",
    "    print(\"æ¨¡å‹è¾“å‡º: \", end=\"\", flush=True)\n",
    "    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´\n",
    "\n",
    "    # è®¾ç½®ç”Ÿæˆå‚æ•°\n",
    "    generation_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=512,  # æœ€å¤§ç”Ÿæˆ512ä¸ªæ–°token\n",
    "        do_sample=True,  # ä½¿ç”¨é‡‡æ ·\n",
    "        top_p=0.7,  # ä½¿ç”¨top-pé‡‡æ ·\n",
    "        temperature=0.95,  # æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§\n",
    "    )\n",
    "\n",
    "    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ¨¡å‹ç”Ÿæˆ\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:  # è¿­ä»£ç”Ÿæˆçš„æ–‡æœ¬æµ\n",
    "        if stop_event.is_set():  # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢ç”Ÿæˆ\n",
    "            print(\"\\nç”Ÿæˆè¢«ç”¨æˆ·åœæ­¢\")\n",
    "            break\n",
    "        generated_text += new_text\n",
    "        print(new_text, end=\"\", flush=True)\n",
    "        history[-1][1] = generated_text  # æ›´æ–°å†å²è®°å½•ä¸­çš„å›å¤\n",
    "        yield history  # é€æ­¥è¿”å›æ›´æ–°çš„å†å²è®°å½•\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\n\\nç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {end_time - start_time:.2f} ç§’\")\n",
    "\n",
    "# å®šä¹‰åœæ­¢ç”Ÿæˆå‡½æ•°\n",
    "def stop_generation():\n",
    "    stop_event.set()  # è®¾ç½®åœæ­¢äº‹ä»¶\n",
    "\n",
    "# ä½¿ç”¨Gradioåˆ›å»ºWebç•Œé¢\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Qwen èŠå¤©æœºå™¨äºº\")\n",
    "    chatbot = gr.Chatbot()  # èŠå¤©ç•Œé¢ç»„ä»¶\n",
    "    msg = gr.Textbox()  # ç”¨æˆ·è¾“å…¥æ–‡æœ¬æ¡†\n",
    "    clear = gr.Button(\"æ¸…é™¤\")  # æ¸…é™¤æŒ‰é’®\n",
    "    stop = gr.Button(\"åœæ­¢ç”Ÿæˆ\")  # åœæ­¢ç”ŸæˆæŒ‰é’®\n",
    "\n",
    "    # è®¾ç½®ç”¨æˆ·è¾“å…¥æäº¤åçš„å¤„ç†æµç¨‹\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)  # æ¸…é™¤æŒ‰é’®åŠŸèƒ½\n",
    "    stop.click(stop_generation, queue=False)  # åœæ­¢ç”ŸæˆæŒ‰é’®åŠŸèƒ½\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"å¯åŠ¨ Gradio ç•Œé¢...\")\n",
    "    demo.queue()  # å¯ç”¨é˜Ÿåˆ—å¤„ç†è¯·æ±‚\n",
    "    demo.launch(root_path='/dsw-576363/proxy/7860/')  # å…¼å®¹é­”æ­æƒ…å†µä¸‹çš„è·¯ç”±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a6122-3e52-41e4-9168-a8de05af22dd",
   "metadata": {},
   "source": [
    "# åœ¨è¿è¡Œä¹‹å‰,æˆ‘ä»¬éœ€è¦å®‰è£… gradio åº“ä¾èµ–ç¯å¢ƒ\n",
    "å½“ç„¶,é™¤äº† gradio ä¹‹å¤–,æˆ‘ä»¬è¿˜æœ‰å¦ä¸€æ¬¾æµè¡Œçš„ python å‰ç«¯å¼€æºåº“å¯ä»¥æ–¹ä¾¿æˆ‘ä»¬çš„å¤§æ¨¡å‹å¯¹è¯åº”ç”¨,å®ƒçš„åå­—å« Streamlit, ç®€å•æ¥è¯´, Streamlitæ˜¯ä¸€ä¸ªPythonåº“ï¼Œç”¨äºå¿«é€Ÿæ„å»ºäº¤äº’å¼Webåº”ç”¨ç¨‹åºã€‚å®ƒæä¾›äº†ä¸€ä¸ªç®€å•çš„APIï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨Pythonä»£ç æ¥åˆ›å»ºWebåº”ç”¨ç¨‹åºï¼Œè€Œæ— éœ€å­¦ä¹ å¤æ‚çš„Webå¼€å‘æŠ€æœ¯. è¿™å¬ä¸Šå»æ˜¯ä¸æ˜¯ä¸ gradio å·®ä¸å¤š? ä½ å¯ä»¥é€‰æ‹©è‡ªå·±å–œæ¬¢çš„ä¸€æ¬¾å‰ç«¯åº“æ¥å®Œæˆå¯¹åº” AI  åº”ç”¨çš„å¼€å‘,å…·ä½“ç»†èŠ‚å¯ä»¥å‚è€ƒå®ƒçš„å®˜æ–¹ç½‘ç«™ https://streamlit.io/, åœ¨è¿™é‡Œ,æˆ‘ä»¬å¯ä»¥è·‘ä¸€ä¸ªæœ€ç®€å•çš„èŠå¤©ç•Œé¢æ¥ä½“éªŒ gradio ä¸ Streamlit å¼€å‘ä¸ä½“éªŒçš„ä¸åŒä¹‹å¤„.\n",
    "\n",
    "æ³¨æ„, åœ¨è¿è¡Œä¹‹å‰,æˆ‘ä»¬éœ€è¦å®‰è£… streamlit åº“ä¾èµ–ç¯å¢ƒ\n",
    "\n",
    "```bash\n",
    "cd /mnt/workspace\n",
    "conda activate ipex\n",
    "pip install streamlit\n",
    "```\n",
    "\n",
    "```python\n",
    "cd /mnt/workspace\n",
    "conda activate ipex\n",
    "streamlit run run_streamlit_stream.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3ccf46-86c6-4695-93f9-fabff5b9dcbf",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:16:33.671755Z",
     "iopub.status.busy": "2024-07-26T04:16:33.671362Z",
     "iopub.status.idle": "2024-07-26T04:16:33.676540Z",
     "shell.execute_reply": "2024-07-26T04:16:33.676021Z",
     "shell.execute_reply.started": "2024-07-26T04:16:33.671732Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/workspace/run_streamlit_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run_streamlit_stream.py\n",
    "\n",
    "\n",
    "# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¨¡å—ï¼Œç”¨äºè®¾ç½®ç¯å¢ƒå˜é‡\n",
    "import os\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ OMP_NUM_THREADS ä¸º 8ï¼Œç”¨äºæ§åˆ¶ OpenMP çº¿ç¨‹æ•°\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# å¯¼å…¥æ—¶é—´æ¨¡å—\n",
    "import time\n",
    "# å¯¼å…¥ Streamlit æ¨¡å—ï¼Œç”¨äºåˆ›å»º Web åº”ç”¨\n",
    "import streamlit as st\n",
    "# ä» transformers åº“ä¸­å¯¼å…¥ AutoTokenizer ç±»\n",
    "from transformers import AutoTokenizer\n",
    "# ä» transformers åº“ä¸­å¯¼å…¥ TextStreamer ç±»\n",
    "from transformers import TextStreamer, TextIteratorStreamer\n",
    "# ä» ipex_llm.transformers åº“ä¸­å¯¼å…¥ AutoModelForCausalLM ç±»\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "# å¯¼å…¥ PyTorch åº“\n",
    "import torch\n",
    "from threading import Thread\n",
    "\n",
    "# æŒ‡å®šæ¨¡å‹è·¯å¾„\n",
    "load_path = \"qwen2chat_int4\"\n",
    "# åŠ è½½ä½æ¯”ç‰¹ç‡æ¨¡å‹\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "# å®šä¹‰ç”Ÿæˆå“åº”å‡½æ•°\n",
    "def generate_response(messages, message_placeholder):\n",
    "    # å°†ç”¨æˆ·çš„æç¤ºè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼\n",
    "    # messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # åº”ç”¨èŠå¤©æ¨¡æ¿å¹¶è¿›è¡Œ token åŒ–\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    # åˆ›å»º TextStreamer å¯¹è±¡ï¼Œè·³è¿‡æç¤ºå’Œç‰¹æ®Šæ ‡è®°\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # ä½¿ç”¨ zip å‡½æ•°åŒæ—¶éå† model_inputs.input_ids å’Œ generated_ids\n",
    "    generation_kwargs = dict(inputs=model_inputs.input_ids, max_new_tokens=512, streamer=streamer)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    return streamer\n",
    "\n",
    "# Streamlit åº”ç”¨éƒ¨åˆ†\n",
    "# è®¾ç½®åº”ç”¨æ ‡é¢˜\n",
    "st.title(\"å¤§æ¨¡å‹èŠå¤©åº”ç”¨\")\n",
    "\n",
    "# åˆå§‹åŒ–èŠå¤©å†å²ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# æ˜¾ç¤ºèŠå¤©å†å²\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# ç”¨æˆ·è¾“å…¥éƒ¨åˆ†\n",
    "if prompt := st.chat_input(\"ä½ æƒ³è¯´ç‚¹ä»€ä¹ˆ?\"):\n",
    "    # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    response  = str()\n",
    "    # åˆ›å»ºç©ºçš„å ä½ç¬¦ç”¨äºæ˜¾ç¤ºç”Ÿæˆçš„å“åº”\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        \n",
    "        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”\n",
    "        streamer = generate_response(st.session_state.messages, message_placeholder)\n",
    "        for text in streamer:\n",
    "            response += text\n",
    "            message_placeholder.markdown(response + \"â–Œ\")\n",
    "    \n",
    "        message_placeholder.markdown(response)\n",
    "    \n",
    "    # å°†åŠ©æ‰‹çš„å“åº”æ·»åŠ åˆ°èŠå¤©å†å²\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118f0f4-4845-4ce3-9243-e478c7160f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gptä¼˜åŒ– \n",
    "#ä¼˜åŒ–è¯´æ˜\n",
    "ç•Œé¢å¸ƒå±€å’Œç¾åŒ–ï¼š\n",
    "\n",
    "è®¾ç½®äº†é¡µé¢é…ç½®ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€å›¾æ ‡å’Œå¸ƒå±€æ–¹å¼ã€‚\n",
    "ä½¿ç”¨äº†è‡ªå®šä¹‰CSSæ¥ç¾åŒ–èƒŒæ™¯å’Œä¾§è¾¹æ ã€‚\n",
    "æ·»åŠ å…¬å¸Logoå’Œç¤¾äº¤åˆ†äº«æŒ‰é’®ã€‚\n",
    "åŠŸèƒ½å¢å¼ºï¼š\n",
    "\n",
    "å¤šæ¨¡å‹é€‰æ‹©ï¼šå…è®¸ç”¨æˆ·ä»å¤šä¸ªé¢„å®šä¹‰æ¨¡å‹ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ã€‚\n",
    "å›¾ç‰‡ä¸Šä¼ ï¼šå…è®¸ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡å¹¶åœ¨é¡µé¢ä¸­æ˜¾ç¤ºã€‚\n",
    "JSONè§£æï¼šæ·»åŠ æŒ‰é’®è§£æèŠå¤©å†å²ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯ä¸ºJSONæ ¼å¼ã€‚\n",
    "èŠå¤©å†å²ä¿å­˜å’ŒåŠ è½½ï¼šæ·»åŠ æŒ‰é’®ä¿å­˜å’ŒåŠ è½½èŠå¤©å†å²åˆ°æœ¬åœ°æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f73bb6a-1c37-47fe-bf91-417df5b3af16",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-26T04:29:30.900118Z",
     "iopub.status.busy": "2024-07-26T04:29:30.899608Z",
     "iopub.status.idle": "2024-07-26T04:29:30.905214Z",
     "shell.execute_reply": "2024-07-26T04:29:30.904711Z",
     "shell.execute_reply.started": "2024-07-26T04:29:30.900097Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /mnt/workspace/run_streamlit_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /mnt/workspace/run_streamlit_stream.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, TextIteratorStreamer\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "from threading import Thread\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "# å®šä¹‰å¯ç”¨çš„æ¨¡å‹é€‰é¡¹\n",
    "model_options = {\n",
    "    \"æ¨¡å‹1\": \"qwen2chat_int4\",\n",
    "    \"æ¨¡å‹2\": \"another_model_path\"\n",
    "}\n",
    "\n",
    "# åŠ è½½å’Œåˆå§‹åŒ–æ¨¡å‹\n",
    "def load_model(model_path):\n",
    "    model = AutoModelForCausalLM.load_low_bit(model_path, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(messages, message_placeholder):\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generation_kwargs = dict(inputs=model_inputs.input_ids, max_new_tokens=512, streamer=streamer)\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    return streamer\n",
    "\n",
    "# Streamlit åº”ç”¨éƒ¨åˆ†\n",
    "st.set_page_config(\n",
    "    page_title=\"å¤šæ¨¡æ€å¤§æ¨¡å‹èŠå¤©åº”ç”¨\",\n",
    "    page_icon=\"ğŸ¤–\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "st.title(\"å¤šæ¨¡æ€å¤§æ¨¡å‹èŠå¤©åº”ç”¨\")\n",
    "st.write(\"ä¸Šä¼ å›¾ç‰‡ã€è¾“å…¥æ–‡æœ¬ï¼Œä¸å¤§æ¨¡å‹äº’åŠ¨å¹¶è·å–å›å¤ã€‚\")\n",
    "\n",
    "# å¤šæ¨¡å‹é€‰æ‹©\n",
    "selected_model = st.sidebar.selectbox(\"é€‰æ‹©æ¨¡å‹\", list(model_options.keys()))\n",
    "model_path = model_options[selected_model]\n",
    "model, tokenizer = load_model(model_path)\n",
    "\n",
    "# åˆå§‹åŒ–èŠå¤©å†å²\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# æ˜¾ç¤ºèŠå¤©å†å²\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# ç”¨æˆ·è¾“å…¥éƒ¨åˆ†\n",
    "if prompt := st.chat_input(\"ä½ æƒ³è¯´ç‚¹ä»€ä¹ˆ?\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    response  = str()\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        streamer = generate_response(st.session_state.messages, message_placeholder)\n",
    "        for text in streamer:\n",
    "            response += text\n",
    "            message_placeholder.markdown(response + \"â–Œ\")\n",
    "\n",
    "        message_placeholder.markdown(response)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# å›¾ç‰‡ä¸Šä¼ åŠŸèƒ½\n",
    "uploaded_image = st.file_uploader(\"ä¸Šä¼ å›¾ç‰‡\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "if uploaded_image:\n",
    "    image = Image.open(uploaded_image)\n",
    "    st.image(image, caption=\"ä¸Šä¼ çš„å›¾ç‰‡\", use_column_width=True)\n",
    "    # å¯ä»¥åœ¨æ­¤å¤„æ·»åŠ å›¾åƒå¤„ç†ä»£ç \n",
    "\n",
    "# JSONè§£æåŠŸèƒ½\n",
    "if st.button(\"è§£æJSON\"):\n",
    "    if st.session_state.messages:\n",
    "        last_message = st.session_state.messages[-1][\"content\"]\n",
    "        try:\n",
    "            json_content = json.loads(last_message)\n",
    "            st.json(json_content)\n",
    "        except json.JSONDecodeError:\n",
    "            st.write(\"æœ€åä¸€æ¡æ¶ˆæ¯ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼\")\n",
    "\n",
    "# èŠå¤©å†å²ä¿å­˜åŠŸèƒ½\n",
    "if st.button(\"ä¿å­˜èŠå¤©å†å²\"):\n",
    "    with open(\"chat_history.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(st.session_state.messages, f, ensure_ascii=False, indent=4)\n",
    "    st.write(\"èŠå¤©å†å²å·²ä¿å­˜\")\n",
    "\n",
    "# åŠ è½½èŠå¤©å†å²åŠŸèƒ½\n",
    "if st.button(\"åŠ è½½èŠå¤©å†å²\"):\n",
    "    if os.path.exists(\"chat_history.json\"):\n",
    "        with open(\"chat_history.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            st.session_state.messages = json.load(f)\n",
    "        st.write(\"èŠå¤©å†å²å·²åŠ è½½\")\n",
    "    else:\n",
    "        st.write(\"æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„èŠå¤©å†å²æ–‡ä»¶\")\n",
    "\n",
    "# ç¾åŒ–é¡µé¢\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .reportview-container {\n",
    "        background: #f0f2f6;\n",
    "    }\n",
    "    .sidebar .sidebar-content {\n",
    "        background: #f0f2f6;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# æ·»åŠ å…¬å¸Logo\n",
    "st.sidebar.image(\"https://your-logo-url.com/logo.png\", width=100)\n",
    "\n",
    "# æ·»åŠ è”ç³»å’Œåˆ†äº«æŒ‰é’®\n",
    "st.sidebar.markdown(\"\"\"\n",
    "    [![Star](https://img.shields.io/github/stars/yourusername/yourrepo.svg?logo=github&style=social)](https://github.com/yourusername/yourrepo)\n",
    "    [![Follow](https://img.shields.io/twitter/follow/yourusername?style=social)](https://twitter.com/yourusername)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d42fac-80f2-4ee4-85ab-9859ae557a80",
   "metadata": {},
   "source": [
    "æ­¤,ä½ å·²ç»å®Œå…¨å…¥é—¨ IPEX-LLM å¯¹å¤§è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²å·¥ç¨‹,ä½† LLM éƒ¨ç½²åªæ˜¯ç¬¬ä¸€æ­¥,åŸºäº LLM çš„åº”ç”¨æ‰æ˜¯å…³é”®,å¯¹äºè¿™æ ·ä¸€æ¬¾èƒ½åœ¨ç«¯ä¾§ä¸Šè¿è¡Œçš„å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ç³»ç»Ÿ,ä½ ä¼šç”¨ä»–ä¼˜åŒ–åçš„å¤§æ¨¡å‹åšäº›ä»€ä¹ˆæœ‰è¶£çš„å¤§æ¨¡å‹åŸç”Ÿåº”ç”¨? æœŸå¾…ä½ çš„æƒ³æ³•èƒ½åˆ›é€ å‡ºä»¤äººæƒŠå¹çš„AIä½œå“.\n",
    "\n",
    "ç¥ä½ å¥½è¿!è‡³"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconve